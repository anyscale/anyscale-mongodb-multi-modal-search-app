{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "import numpy as np\n",
    "import os\n",
    "import pymongo\n",
    "import pyarrow as pa\n",
    "import ray\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pyarrow import csv\n",
    "from pymongo import MongoClient, ASCENDING, DESCENDING, UpdateOne\n",
    "from pymongo.operations import SearchIndexModel, IndexModel\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from ray.util.accelerators import NVIDIA_TESLA_A10G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate import  Accelerator\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "Accelerator().free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 23:59:41,736\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.2 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from vllm import LLM\n",
    "from vllm.multimodal.image import ImageFeatureData, ImagePixelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 23:59:42 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='llava-hf/llava-v1.6-mistral-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-v1.6-mistral-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=14224, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=llava-hf/llava-v1.6-mistral-7b-hf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 23:59:44 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 06-25 23:59:52 model_runner.py:160] Loading model weights took 14.1020 GB\n",
      "INFO 06-25 23:59:57 gpu_executor.py:83] # GPU blocks: 1316, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "# llm = LLM(\n",
    "#     model=\"llava-hf/llava-1.5-7b-hf\",\n",
    "#     image_input_type=\"pixel_values\",\n",
    "#     image_token_id=32000,\n",
    "#     image_input_shape=\"1,3,336,336\",\n",
    "#     image_feature_size=576,\n",
    "#     disable_image_processor=False,\n",
    "# )\n",
    "llm = LLM(\n",
    "    model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    # image_input_type=\"pixel_values\",\n",
    "    # image_token_id=32000,\n",
    "    # image_input_shape=\"1,3,336,336\",\n",
    "    # image_feature_size=576,\n",
    "    # disable_image_processor=False,\n",
    "    **{\n",
    "        \"trust_remote_code\": True,\n",
    "        \"enable_lora\": False,\n",
    "        \"max_num_seqs\": 1,\n",
    "        \"max_model_len\": 14224,\n",
    "        \"gpu_memory_utilization\": 0.85,\n",
    "        \"image_input_type\": \"pixel_values\",\n",
    "        \"image_token_id\": 32000,\n",
    "        \"image_input_shape\": \"1,3,336,336\",\n",
    "        \"image_feature_size\": 1176,\n",
    "        \"enforce_eager\": True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_image_processor=False\n",
    "\n",
    "prompt = \"<image>\" * 1176 + (\n",
    "    \"\\nUSER: What is the content of this image?\\nASSISTANT:\")\n",
    "\n",
    "if disable_image_processor:\n",
    "    image = torch.load(\"images/stop_sign_pixel_values.pt\")\n",
    "else:\n",
    "    image = Image.open(\"stop_sign.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-26 00:00:04 llava_next.py:98] Dynamic image shape is currently not supported. Resizing input image to (336, 336).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 1316.36 toks/s, output: 17.64 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate({\n",
    "    \"prompt\": prompt,\n",
    "    \"multi_modal_data\": ImagePixelData(image),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CompletionOutput(index=0, text=' The image shows a red stop sign with the word \"STOP\" written in white', token_ids=[415, 3469, 4370, 264, 2760, 2115, 1492, 395, 272, 1707, 345, 26599, 28739, 4241, 297, 3075], cumulative_logprob=-4.296915684157284, logprobs=None, finish_reason=length, stop_reason=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralvLLM:\n",
    "    def __init__(self):\n",
    "        self.llm = LLM(\n",
    "            model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            max_model_len=4096,\n",
    "            skip_tokenizer_init=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = MistralvLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    n=1,\n",
    "    presence_penalty=0,\n",
    "    frequency_penalty=0,\n",
    "    repetition_penalty=1,\n",
    "    length_penalty=1,\n",
    "    top_p=1,\n",
    "    top_k=-1,\n",
    "    temperature=0,\n",
    "    use_beam_search=False,\n",
    "    ignore_eos=False,\n",
    "    max_tokens=2048,\n",
    "    seed=None,\n",
    "    detokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        )\n",
    "input_text = \"What is the capital of france?\"\n",
    "input_tokens = tokenizer.apply_chat_template(\n",
    "    conversation=[{\"role\": \"user\", \"content\": input_text}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"np\",\n",
    ").squeeze()\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.llm.generate(prompt_token_ids=input_tokens.tolist(), sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[0].outputs[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(response[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
